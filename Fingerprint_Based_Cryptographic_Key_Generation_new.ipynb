{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 10709357,
          "sourceType": "datasetVersion",
          "datasetId": 6637356
        }
      ],
      "dockerImageVersionId": 31041,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Fingerprint Based Cryptographic Key Generation new",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PhilipGunjari12/Projects/blob/main/Fingerprint_Based_Cryptographic_Key_Generation_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "T439GCP3teTA"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "gpkchvp_data_sets_path = kagglehub.dataset_download('gpkchvp/data-sets')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "6sSyc9lVteTF"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install packages**"
      ],
      "metadata": {
        "id": "rl7CfpTfteTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasketch\n",
        "!pip install reedsolo\n",
        "!pip install cryptography"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-08T04:43:20.979081Z",
          "iopub.execute_input": "2025-07-08T04:43:20.979428Z",
          "iopub.status.idle": "2025-07-08T04:43:29.784217Z",
          "shell.execute_reply.started": "2025-07-08T04:43:20.979392Z",
          "shell.execute_reply": "2025-07-08T04:43:29.78335Z"
        },
        "id": "tM_l-B84teTJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import libraries**"
      ],
      "metadata": {
        "id": "LRRkUpwUteTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import reedsolo\n",
        "import hashlib\n",
        "from collections import defaultdict\n",
        "import tensorflow as tf\n",
        "from datasketch import MinHash\n",
        "from cryptography.hazmat.primitives.asymmetric import ec\n",
        "from cryptography.hazmat.primitives import serialization\n",
        "from cryptography.hazmat.primitives.kdf.hkdf import HKDF\n",
        "from cryptography.hazmat.primitives.hashes import SHA256\n",
        "from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes\n",
        "from cryptography.hazmat.primitives import hashes\n",
        "from cryptography.hazmat.primitives.asymmetric import ec\n",
        "from cryptography.hazmat.backends import default_backend\n",
        "from tensorflow.keras.applications.mobilenet_v3 import preprocess_input\n",
        "from tensorflow.keras.applications import MobileNetV3Small\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Input\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-08T04:43:29.786002Z",
          "iopub.execute_input": "2025-07-08T04:43:29.786241Z",
          "iopub.status.idle": "2025-07-08T04:43:29.792351Z",
          "shell.execute_reply.started": "2025-07-08T04:43:29.786215Z",
          "shell.execute_reply": "2025-07-08T04:43:29.791655Z"
        },
        "id": "Ax81ukVJteTO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Contents Of DB1_A DATASET**"
      ],
      "metadata": {
        "id": "VDFxaYpPteTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path_a = \"/kaggle/input/data-sets/74034_3_En_4_MOESM1_ESM/FVC2004/Dbs/DB1_A/\"\n",
        "dataset_path_b = \"/kaggle/input/data-sets/74034_3_En_4_MOESM1_ESM/FVC2004/Dbs/DB1_B/\"\n",
        "def extract_two_numbers(filename):\n",
        "    parts = filename.split('_')\n",
        "    return (int(parts[0]), int(parts[1].split('.')[0]))\n",
        "\n",
        "sorted_filenames_a = sorted(\n",
        "    [f for f in os.listdir(dataset_path_a) if f.endswith('.tif')],\n",
        "    key=extract_two_numbers\n",
        ")\n",
        "\n",
        "print(\"Contents of dataset folder:\", sorted_filenames_a)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-08T04:43:29.793193Z",
          "iopub.execute_input": "2025-07-08T04:43:29.79343Z",
          "iopub.status.idle": "2025-07-08T04:43:29.816358Z",
          "shell.execute_reply.started": "2025-07-08T04:43:29.79341Z",
          "shell.execute_reply": "2025-07-08T04:43:29.815705Z"
        },
        "id": "TZovUR6pteTP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Contents Of DB1_B DATASET**"
      ],
      "metadata": {
        "id": "LKez4j3NteTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_filenames_b = sorted(\n",
        "    [f for f in os.listdir(dataset_path_b) if f.endswith('.tif')],\n",
        "    key=extract_two_numbers\n",
        ")\n",
        "\n",
        "print(\"Contents of dataset folder:\", sorted_filenames_b)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-08T04:43:29.817955Z",
          "iopub.execute_input": "2025-07-08T04:43:29.818152Z",
          "iopub.status.idle": "2025-07-08T04:43:29.827751Z",
          "shell.execute_reply.started": "2025-07-08T04:43:29.818138Z",
          "shell.execute_reply": "2025-07-08T04:43:29.827054Z"
        },
        "id": "rXZ4862ZteTR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load DB1_A & DB1_B Datasets**"
      ],
      "metadata": {
        "id": "POJdj6nEteTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_rgb_images_from_files(file_list, folder_path, img_size=(128, 128)):\n",
        "    images = []\n",
        "    for filename in file_list:\n",
        "        img_path = os.path.join(folder_path, filename)\n",
        "        img_gray = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "        if img_gray is None:\n",
        "            continue\n",
        "        img_gray = cv2.resize(img_gray, img_size)\n",
        "        img_rgb = cv2.cvtColor(img_gray, cv2.COLOR_GRAY2RGB)\n",
        "        images.append(img_rgb)\n",
        "    return np.array(images)\n",
        "\n",
        "X = load_rgb_images_from_files(sorted_filenames_a, dataset_path_a)\n",
        "X = X / 255.0  # Normalize\n",
        "print(f\"Loaded {len(X)} RGB images with shape {X.shape}\")\n",
        "\n",
        "Y = load_rgb_images_from_files(sorted_filenames_b, dataset_path_b)\n",
        "Y = Y / 255.0  # Normalize\n",
        "print(f\"Loaded {len(Y)} RGB images with shape {Y.shape}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-08T04:43:29.828557Z",
          "iopub.execute_input": "2025-07-08T04:43:29.828805Z",
          "iopub.status.idle": "2025-07-08T04:43:31.650621Z",
          "shell.execute_reply.started": "2025-07-08T04:43:29.828782Z",
          "shell.execute_reply": "2025-07-08T04:43:31.649828Z"
        },
        "id": "avdxn8R9teTR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Display DB1_A DATASET**"
      ],
      "metadata": {
        "id": "t9lFdyIXteTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "images_by_person_a = defaultdict(list)\n",
        "for fname, img in zip(sorted_filenames_a, X):\n",
        "    person_id = fname.split('_')[0]\n",
        "    images_by_person_a[person_id].append(img)\n",
        "\n",
        "for idx, (person_id, imgs) in enumerate(images_by_person_a.items()):\n",
        "    if idx >= 1:\n",
        "        print(\"\\nChange the number to display more persons...\\n\")\n",
        "        break\n",
        "\n",
        "    print(f\"\\nPerson ID: {person_id} | Total Images: {len(imgs)}\")\n",
        "\n",
        "    plt.figure(figsize=(15, 2))\n",
        "    for i, img in enumerate(imgs):\n",
        "        plt.subplot(1, len(imgs), i+1)\n",
        "        plt.imshow(img)\n",
        "        plt.axis('off')\n",
        "        plt.title(f\"{i+1}\")\n",
        "    plt.suptitle(f\" Fingerprints of Person {person_id}\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-08T04:43:31.651508Z",
          "iopub.execute_input": "2025-07-08T04:43:31.651832Z",
          "iopub.status.idle": "2025-07-08T04:43:32.148387Z",
          "shell.execute_reply.started": "2025-07-08T04:43:31.651804Z",
          "shell.execute_reply": "2025-07-08T04:43:32.147606Z"
        },
        "id": "UAjgrANlteTS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Display DB1_A DATASET**"
      ],
      "metadata": {
        "id": "3CTxLsMpteTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "images_by_person_b = defaultdict(list)\n",
        "for fname, img in zip(sorted_filenames_b, Y):\n",
        "    person_id = fname.split('_')[0]\n",
        "    images_by_person_b[person_id].append(img)\n",
        "\n",
        "\n",
        "for idx, (person_id, imgs) in enumerate(images_by_person_b.items()):\n",
        "    if idx >= 1:\n",
        "        print(\"\\nChange the number to display more persons...\\n\")\n",
        "        break\n",
        "\n",
        "    print(f\"\\nPerson ID: {person_id} | Total Images: {len(imgs)}\")\n",
        "\n",
        "    plt.figure(figsize=(15, 2))\n",
        "    for i, img in enumerate(imgs):\n",
        "        plt.subplot(1, len(imgs), i+1)\n",
        "        plt.imshow(img)\n",
        "        plt.axis('off')\n",
        "        plt.title(f\"{i+1}\")\n",
        "    plt.suptitle(f\" Fingerprints of Person {person_id}\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-08T04:43:32.149174Z",
          "iopub.execute_input": "2025-07-08T04:43:32.149377Z",
          "iopub.status.idle": "2025-07-08T04:43:32.481345Z",
          "shell.execute_reply.started": "2025-07-08T04:43:32.149362Z",
          "shell.execute_reply": "2025-07-08T04:43:32.480688Z"
        },
        "id": "K8RIN-hwteTS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MobileNetV3 Model for Feature Extraction**"
      ],
      "metadata": {
        "id": "3wL1WcRVteTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def build_mobilenetv3_feature_extractor(input_shape=(128, 128, 3)):\n",
        "    #base_model = MobileNetV2(input_shape=input_shape, include_top=False, weights=None)\n",
        "    base_model = MobileNetV3Small(input_shape=(128, 128, 3), include_top=False, weights=None)\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(64, activation='relu')(x)\n",
        "    model = Model(inputs=base_model.input, outputs=x)\n",
        "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "    return model\n",
        "\n",
        "model = build_mobilenetv3_feature_extractor()\n",
        "model.summary()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-08T04:43:32.482203Z",
          "iopub.execute_input": "2025-07-08T04:43:32.482731Z",
          "iopub.status.idle": "2025-07-08T04:43:36.867171Z",
          "shell.execute_reply.started": "2025-07-08T04:43:32.482702Z",
          "shell.execute_reply": "2025-07-08T04:43:36.866492Z"
        },
        "id": "QVVimKpRteTT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train DB1_A Dataset**"
      ],
      "metadata": {
        "id": "bU9Mzl53teTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.random.rand(len(X), 64)\n",
        "model.fit(X, y, epochs=200, batch_size=16, verbose=1)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-08T04:43:36.867878Z",
          "iopub.execute_input": "2025-07-08T04:43:36.868089Z",
          "iopub.status.idle": "2025-07-08T04:46:23.390255Z",
          "shell.execute_reply.started": "2025-07-08T04:43:36.868074Z",
          "shell.execute_reply": "2025-07-08T04:46:23.389641Z"
        },
        "id": "strKlc_yteTU"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train DB1_B Dataset**"
      ],
      "metadata": {
        "id": "Pafwq0JjteTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y2 = np.random.rand(len(Y), 64)\n",
        "model.fit(Y, y2, epochs=200, batch_size=16, verbose=1)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-08T04:46:23.392464Z",
          "iopub.execute_input": "2025-07-08T04:46:23.392659Z",
          "iopub.status.idle": "2025-07-08T04:46:44.985279Z",
          "shell.execute_reply.started": "2025-07-08T04:46:23.392645Z",
          "shell.execute_reply": "2025-07-08T04:46:44.984734Z"
        },
        "id": "gv9FWAbateTU"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature vectors of DB1_A**"
      ],
      "metadata": {
        "id": "qSUcCkYvteTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_feature_vector(image_rgb):\n",
        "    img = cv2.resize(image_rgb, (128, 128))\n",
        "    img = img.reshape(1, 128, 128, 3) / 255.0\n",
        "    return model.predict(img, verbose=0).flatten()\n",
        "\n",
        "person_features_a = defaultdict(list)\n",
        "\n",
        "for person_id, imgs in images_by_person_a.items():\n",
        "    for img in imgs:\n",
        "        vec = extract_feature_vector(img)\n",
        "        person_features_a[person_id].append(vec)\n",
        "\n",
        "for person_id in person_features_a:\n",
        "    person_features_a[person_id] = np.array(person_features_a[person_id])\n",
        "\n",
        "sample_id = list(person_features_a.keys())[0]\n",
        "print(f\"\\nFeature Vectors for Person {sample_id} | Total: {len(person_features_a[sample_id])}\")\n",
        "for i, vec in enumerate(person_features_a[sample_id]):\n",
        "    print(f\"  Feature Vector {i+1}: {vec}\\n\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-08T04:46:44.986087Z",
          "iopub.execute_input": "2025-07-08T04:46:44.98638Z",
          "iopub.status.idle": "2025-07-08T04:47:48.048987Z",
          "shell.execute_reply.started": "2025-07-08T04:46:44.986355Z",
          "shell.execute_reply": "2025-07-08T04:47:48.048283Z"
        },
        "id": "9hiaJwUEteTU"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature vectors of DB1_B**"
      ],
      "metadata": {
        "id": "zOnLWdL0teTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_feature_vector(image_rgb):\n",
        "    img = cv2.resize(image_rgb, (128, 128))\n",
        "    img = img.reshape(1, 128, 128, 3) / 255.0\n",
        "    return model.predict(img, verbose=0).flatten()\n",
        "\n",
        "person_features_b = defaultdict(list)\n",
        "\n",
        "for person_id, imgs in images_by_person_b.items():\n",
        "    for img in imgs:\n",
        "        vec = extract_feature_vector(img)\n",
        "        person_features_b[person_id].append(vec)\n",
        "\n",
        "for person_id in person_features_b:\n",
        "    person_features_b[person_id] = np.array(person_features_b[person_id])\n",
        "\n",
        "sample_id = list(person_features_b.keys())[0]\n",
        "print(f\"\\nFeature Vectors for Person {sample_id} | Total: {len(person_features_b[sample_id])}\")\n",
        "for i, vec in enumerate(person_features_b[sample_id]):\n",
        "    print(f\"  Feature Vector {i+1}: {vec}\\n\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-08T04:47:48.049739Z",
          "iopub.execute_input": "2025-07-08T04:47:48.049959Z",
          "iopub.status.idle": "2025-07-08T04:47:53.894738Z",
          "shell.execute_reply.started": "2025-07-08T04:47:48.049935Z",
          "shell.execute_reply": "2025-07-08T04:47:53.893956Z"
        },
        "id": "0ZsL1V_RteTV"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Quantization and Secure Key Binding with Reed-Solomon Error Correction**"
      ],
      "metadata": {
        "id": "SjNDf9IJteTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def quantize_features(fv, decimals=2):\n",
        "    return np.round(fv, decimals=decimals)\n",
        "\n",
        "rs = reedsolo.RSCodec(32)\n",
        "\n",
        "def enroll(feature_vector):\n",
        "    quantized = quantize_features(feature_vector)\n",
        "    data_bytes = bytearray(np.clip((quantized * 100).astype(int), 0, 255).tolist()[:223])\n",
        "    key = os.urandom(16)\n",
        "    codeword = rs.encode(key)\n",
        "    helper = bytearray(a ^ b for a, b in zip(codeword, data_bytes))\n",
        "    return helper, hashlib.sha256(key).hexdigest()\n",
        "\n",
        "def authenticate(feature_vector, helper, stored_hash):\n",
        "    quantized = quantize_features(feature_vector)\n",
        "    data_bytes = bytearray(np.clip((quantized * 100).astype(int), 0, 255).tolist()[:223])\n",
        "    codeword = bytearray(a ^ b for a, b in zip(helper, data_bytes))\n",
        "    try:\n",
        "        recovered_key = rs.decode(codeword)[0]\n",
        "        recovered_hash = hashlib.sha256(recovered_key).hexdigest()\n",
        "        return recovered_hash == stored_hash\n",
        "    except:\n",
        "        return False\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-08T04:47:53.895593Z",
          "iopub.execute_input": "2025-07-08T04:47:53.895967Z",
          "iopub.status.idle": "2025-07-08T04:47:53.902833Z",
          "shell.execute_reply.started": "2025-07-08T04:47:53.895942Z",
          "shell.execute_reply": "2025-07-08T04:47:53.90215Z"
        },
        "id": "PhOKuBPZteTV"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key Generation Function**"
      ],
      "metadata": {
        "id": "1GidtrDTteTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_stable_key(feature_vector, num_hashes=149):\n",
        "\n",
        "    feature_vector = np.array(feature_vector)\n",
        "    feature_vector = (feature_vector - feature_vector.min()) / (feature_vector.max() - feature_vector.min() + 1e-8)\n",
        "\n",
        "    minhash = MinHash(num_perm=num_hashes)\n",
        "    for val in feature_vector:\n",
        "        val_str = f\"{val:.6f}\"\n",
        "        minhash.update(val_str.encode(\"utf-8\"))\n",
        "\n",
        "    minhash_digest = minhash.digest()\n",
        "    rs = reedsolo.RSCodec(10)\n",
        "    encoded_digest = rs.encode(minhash_digest)\n",
        "\n",
        "    stable_key = hashlib.sha256(encoded_digest).hexdigest()\n",
        "    return stable_key, encoded_digest\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-08T04:47:53.903538Z",
          "iopub.execute_input": "2025-07-08T04:47:53.903757Z",
          "iopub.status.idle": "2025-07-08T04:47:53.924594Z",
          "shell.execute_reply.started": "2025-07-08T04:47:53.903742Z",
          "shell.execute_reply": "2025-07-08T04:47:53.924066Z"
        },
        "id": "LmjVxi35teTW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generate Stable Keys for DB1_A**"
      ],
      "metadata": {
        "id": "kvf30uchteTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "stable_keys_per_person_a = defaultdict(list)\n",
        "\n",
        "for person_id, feature_array in person_features_a.items():\n",
        "    for i, feature_vec in enumerate(feature_array):\n",
        "        stable_key, _ = generate_stable_key(feature_vec)\n",
        "        stable_keys_per_person_a[person_id].append(stable_key)\n",
        "\n",
        "print(f\" Generated stable keys for {len(stable_keys_per_person_a)} persons.\\n\")\n",
        "\n",
        "sample_id = list(stable_keys_per_person_a.keys())[0]\n",
        "print(f\" Person ID: {sample_id} | Total Keys: {len(stable_keys_per_person_a[sample_id])}\")\n",
        "for i, key in enumerate(stable_keys_per_person_a[sample_id]):\n",
        "    print(f\"  Key {i+1}: {key}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-08T04:47:53.925534Z",
          "iopub.execute_input": "2025-07-08T04:47:53.925794Z",
          "iopub.status.idle": "2025-07-08T04:47:57.066466Z",
          "shell.execute_reply.started": "2025-07-08T04:47:53.925772Z",
          "shell.execute_reply": "2025-07-08T04:47:57.065758Z"
        },
        "id": "LsbyA0bmteTW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generate Stable Keys for DB1_B**"
      ],
      "metadata": {
        "id": "K-RVEQtzteTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stable_keys_per_person_b = defaultdict(list)\n",
        "\n",
        "for person_id, feature_array in person_features_b.items():\n",
        "    for i, feature_vec in enumerate(feature_array):\n",
        "        stable_key, _ = generate_stable_key(feature_vec)\n",
        "        stable_keys_per_person_b[person_id].append(stable_key)\n",
        "\n",
        "print(f\" Generated stable keys for {len(stable_keys_per_person_b)} persons.\\n\")\n",
        "\n",
        "sample_id = list(stable_keys_per_person_b.keys())[0]\n",
        "print(f\" Person ID: {sample_id} | Total Keys: {len(stable_keys_per_person_b[sample_id])}\")\n",
        "for i, key in enumerate(stable_keys_per_person_b[sample_id]):\n",
        "    print(f\"  Key {i+1}: {key}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-08T04:47:57.067168Z",
          "iopub.execute_input": "2025-07-08T04:47:57.067367Z",
          "iopub.status.idle": "2025-07-08T04:47:57.405607Z",
          "shell.execute_reply.started": "2025-07-08T04:47:57.067352Z",
          "shell.execute_reply": "2025-07-08T04:47:57.404885Z"
        },
        "id": "Cfre07MgteTX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hamming Distance for Genuine Fingerprint Pairs**"
      ],
      "metadata": {
        "id": "l1Tn-74TteTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from itertools import combinations\n",
        "\n",
        "def hamming_distance(hex1, hex2):\n",
        "    bin1 = bin(int(hex1, 16))[2:].zfill(256)\n",
        "    bin2 = bin(int(hex2, 16))[2:].zfill(256)\n",
        "    return sum(c1 != c2 for c1, c2 in zip(bin1, bin2))\n",
        "\n",
        "print(\"\\n Genuine Hamming Distances (same person):\\n\")\n",
        "\n",
        "for person_id, keys in stable_keys_per_person_a.items():\n",
        "    print(f\" Person {person_id}:\")\n",
        "\n",
        "    for i, j in combinations(range(len(keys)), 2):  # all pairs\n",
        "        hd = hamming_distance(keys[i], keys[j])\n",
        "        print(f\"   Key {i+1} vs Key {j+1} → Hamming Distance: {hd}\")\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-08T04:47:57.406556Z",
          "iopub.execute_input": "2025-07-08T04:47:57.406805Z",
          "iopub.status.idle": "2025-07-08T04:47:57.474784Z",
          "shell.execute_reply.started": "2025-07-08T04:47:57.406787Z",
          "shell.execute_reply": "2025-07-08T04:47:57.474028Z"
        },
        "id": "PEvkD9EoteTX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Impostor Hamming Distances (first key of different persons)**"
      ],
      "metadata": {
        "id": "hYS89tTFteTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n Impostor Hamming Distances (first key of different persons):\\n\")\n",
        "\n",
        "# Collect first key from each person\n",
        "first_keys = {pid: keys[0] for pid, keys in stable_keys_per_person_b.items() if len(keys) > 0}\n",
        "\n",
        "for (pid1, key1), (pid2, key2) in combinations(first_keys.items(), 2):\n",
        "    hd = hamming_distance(key1, key2)\n",
        "    print(f\" Person {pid1} vs Person {pid2} → Hamming Distance: {hd}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-08T04:47:57.475513Z",
          "iopub.execute_input": "2025-07-08T04:47:57.475744Z",
          "iopub.status.idle": "2025-07-08T04:47:57.481271Z",
          "shell.execute_reply.started": "2025-07-08T04:47:57.475729Z",
          "shell.execute_reply": "2025-07-08T04:47:57.480597Z"
        },
        "id": "P4dU5X71teTY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mapping Of DB1_A & DB1_B IDs**"
      ],
      "metadata": {
        "id": "UYc2m9fgteTZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "standardized_keys_a = {}\n",
        "for pid_str, keys in stable_keys_per_person_a.items():\n",
        "    pid = int(pid_str)\n",
        "    if 1 <= pid <= 10:\n",
        "        new_pid = str(100 + pid)\n",
        "        standardized_keys_a[new_pid] = keys\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-08T04:47:57.481886Z",
          "iopub.execute_input": "2025-07-08T04:47:57.482075Z",
          "iopub.status.idle": "2025-07-08T04:47:57.501402Z",
          "shell.execute_reply.started": "2025-07-08T04:47:57.48206Z",
          "shell.execute_reply": "2025-07-08T04:47:57.500714Z"
        },
        "id": "n8bI_S2lteTg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n Remapped person IDs in DB1_A:\", list(standardized_keys_a.keys()))\n",
        "print(\" Person IDs in DB1_B:\", list(stable_keys_per_person_b.keys()))\n",
        "\n",
        "# Print key counts for common IDs\n",
        "common_ids = sorted(set(standardized_keys_a) & set(stable_keys_per_person_b))\n",
        "print(f\" Common IDs: {common_ids}\\n\")\n",
        "\n",
        "for pid in common_ids:\n",
        "    len_a = len(standardized_keys_a[pid])\n",
        "    len_b = len(stable_keys_per_person_b[pid])\n",
        "    print(f\" Person {pid}: A = {len_a} keys, B = {len_b} keys\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-08T04:47:57.502122Z",
          "iopub.execute_input": "2025-07-08T04:47:57.502279Z",
          "iopub.status.idle": "2025-07-08T04:47:57.519463Z",
          "shell.execute_reply.started": "2025-07-08T04:47:57.502267Z",
          "shell.execute_reply": "2025-07-08T04:47:57.518954Z"
        },
        "id": "LYTGyUlZteTg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EER Computation**"
      ],
      "metadata": {
        "id": "LkkeBsdDteTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_eer_and_print(genuine_distances, impostor_distances, key_length):\n",
        "    if not genuine_distances or not impostor_distances:\n",
        "        print(f\" Couldn't compute EER for key length {key_length} (empty distance list)\")\n",
        "        return\n",
        "\n",
        "    min_th = min(genuine_distances + impostor_distances)\n",
        "    max_th = max(genuine_distances + impostor_distances)\n",
        "    thresholds = range(min_th, max_th + 1)\n",
        "\n",
        "    eer = None\n",
        "    eer_threshold = None\n",
        "\n",
        "    print(f\"\\n EER Calculation for Key Length = {key_length} bits:\\n\")\n",
        "    print(\"Threshold |   FAR (%) |   FRR (%) | |FAR - FRR|\")\n",
        "\n",
        "    for th in thresholds:\n",
        "        FAR = sum(d <= th for d in impostor_distances) / len(impostor_distances)\n",
        "        FRR = sum(d > th for d in genuine_distances) / len(genuine_distances)\n",
        "        diff = abs(FAR - FRR)\n",
        "\n",
        "        print(f\"{th:9} | {FAR*100:9.2f} | {FRR*100:9.2f} | {diff:.4f}\")\n",
        "\n",
        "        if eer is None or diff < abs(eer[0] - eer[1]):\n",
        "            eer = (FAR, FRR)\n",
        "            eer_threshold = th\n",
        "\n",
        "    print(f\"\\n Equal Error Rate (EER) for Key Length {key_length}:\")\n",
        "    print(f\"   → Threshold = {eer_threshold}\")\n",
        "    print(f\"   → FAR = {eer[0]*100:.2f}%, FRR = {eer[1]*100:.2f}%\")\n",
        "    print(f\"   → EER = {(eer[0]*100 + eer[1]*100)/2:.2f}%\")\n",
        "\n",
        "genuine_distances = []\n",
        "impostor_distances = []\n",
        "\n",
        "for pid in common_ids:\n",
        "    keys_a = standardized_keys_a[pid]\n",
        "    keys_b = stable_keys_per_person_b[pid]\n",
        "    for k1 in keys_a:\n",
        "        for k2 in keys_b:\n",
        "            genuine_distances.append(hamming_distance(k1, k2))\n",
        "\n",
        "for pid1, pid2 in combinations(common_ids, 2):\n",
        "    k1 = standardized_keys_a[pid1][0]\n",
        "    k2 = stable_keys_per_person_b[pid2][0]\n",
        "    impostor_distances.append(hamming_distance(k1, k2))\n",
        "\n",
        "\n",
        "current_key_length = 128\n",
        "compute_eer_and_print(genuine_distances, impostor_distances, current_key_length)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-08T04:57:46.214591Z",
          "iopub.execute_input": "2025-07-08T04:57:46.215243Z",
          "iopub.status.idle": "2025-07-08T04:57:46.237783Z",
          "shell.execute_reply.started": "2025-07-08T04:57:46.215219Z",
          "shell.execute_reply": "2025-07-08T04:57:46.237112Z"
        },
        "id": "MNpQYYR5teTh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Entropy Computation**"
      ],
      "metadata": {
        "id": "gfr4Vcn_teTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def shannon_entropy(p):\n",
        "    if p == 0 or p == 1:\n",
        "        return 0.0\n",
        "    return -p * math.log2(p) - (1 - p) * math.log2(1 - p)\n",
        "\n",
        "def compute_entropy_from_keys(hex_keys):\n",
        "    binary_keys = [bin(int(k, 16))[2:].zfill(256) for k in hex_keys]\n",
        "    bit_columns = list(zip(*binary_keys))  # bit i = all i-th bits from keys\n",
        "\n",
        "    entropies = []\n",
        "    for col in bit_columns:\n",
        "        ones = col.count('1')\n",
        "        total = len(col)\n",
        "        p_one = ones / total\n",
        "        entropy = shannon_entropy(p_one)\n",
        "        entropies.append(entropy)\n",
        "\n",
        "    return sum(entropies), entropies\n",
        "\n",
        "all_keys_a = [k for keys in stable_keys_per_person_a.values() for k in keys]\n",
        "all_keys_b = [k for keys in stable_keys_per_person_b.values() for k in keys]\n",
        "\n",
        "entropy_a, bitwise_a = compute_entropy_from_keys(all_keys_a)\n",
        "entropy_b, bitwise_b = compute_entropy_from_keys(all_keys_b)\n",
        "\n",
        "print(f\" DB1_A Total Entropy: {entropy_a:.4f} bits\")\n",
        "print(f\" DB1_B Total Entropy: {entropy_b:.4f} bits\")\n",
        "print(f\" Avg Bitwise Entropy A: {sum(bitwise_a)/256:.4f}\")\n",
        "print(f\" Avg Bitwise Entropy B: {sum(bitwise_b)/256:.4f}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-08T04:47:57.550443Z",
          "iopub.execute_input": "2025-07-08T04:47:57.55065Z",
          "iopub.status.idle": "2025-07-08T04:47:57.582217Z",
          "shell.execute_reply.started": "2025-07-08T04:47:57.55063Z",
          "shell.execute_reply": "2025-07-08T04:47:57.58155Z"
        },
        "id": "tyJxzc94teTh"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}